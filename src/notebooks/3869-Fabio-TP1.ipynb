{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuração Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from -r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements/requirements_tp1.txt (line 1)) (2.2.1)\n",
      "Requirement already satisfied: numpy in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from -r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements/requirements_tp1.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: filelock in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements/requirements_tp1.txt (line 1)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements/requirements_tp1.txt (line 1)) (4.10.0)\n",
      "Requirement already satisfied: sympy in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements/requirements_tp1.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: networkx in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements/requirements_tp1.txt (line 1)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements/requirements_tp1.txt (line 1)) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements/requirements_tp1.txt (line 1)) (2024.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements/requirements_tp1.txt (line 1)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements/requirements_tp1.txt (line 1)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements/requirements_tp1.txt (line 1)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements/requirements_tp1.txt (line 1)) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements/requirements_tp1.txt (line 1)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements/requirements_tp1.txt (line 1)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements/requirements_tp1.txt (line 1)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements/requirements_tp1.txt (line 1)) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements/requirements_tp1.txt (line 1)) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements/requirements_tp1.txt (line 1)) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements/requirements_tp1.txt (line 1)) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements/requirements_tp1.txt (line 1)) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements/requirements_tp1.txt (line 1)) (12.4.99)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from jinja2->torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements/requirements_tp1.txt (line 1)) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from sympy->torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements/requirements_tp1.txt (line 1)) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "if str(pathlib.Path().resolve().name) == \"notebooks\":\n",
    "    root = str(pathlib.Path().resolve().parent.parent)+'/'\n",
    "    sys.path.append(root)\n",
    "    requirements_path = root + 'requirements/requirements_tp1.txt'\n",
    "else:\n",
    "    ! git clone https://github.com/Fabio-Trindade/Eng-Aprendizado-Maquina.git\n",
    "    root = str(pathlib.Path().resolve())\n",
    "    src_path = root + '/Eng-Aprendizado-Maquina/'\n",
    "    requirements_path = src_path + 'requirements/requirements_tp1.txt'\n",
    "    sys.path.append(root + '/Eng-Aprendizado-Maquina/')\n",
    "\n",
    "! pip install -r $requirements_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.datasets.spaceship_titanic_dataset import SpaceshipTitanicDataset\n",
    "from src.models.spaceship_model import SpaceShipModel\n",
    "from src.pre_processors.pre_processor_spaceship import PreProcessorSpaceship\n",
    "from src.transforms.to_torch_tensor import NumpyToTorchTensor\n",
    "from src.utils.util_path import UtilPath\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from src.constants.KPaths import KPaths "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Os dados\n",
    "Os dados são constituídos das seguintes informações (retiradas em [kaggle](https://www.kaggle.com/competitions/spaceship-titanic/data)):\n",
    "* __PassengerId__ - A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.\n",
    "* __HomePlanet__ - The planet the passenger departed from, typically their planet of permanent residence.\n",
    "* __CryoSleep__ - Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.\n",
    "* __Cabin__ - The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.\n",
    "* __Destination__ - The planet the passenger will be debarking to.\n",
    "* __Age__ - The age of the passenger.\n",
    "* __VIP__ - Whether the passenger has paid for special VIP service during the voyage.\n",
    "* __RoomService, FoodCourt, ShoppingMall, Spa, VRDeck__ - Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities.\n",
    "* __Name__ - The first and last names of the passenger.\n",
    "* __Transported__ - Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Age</th>\n",
       "      <th>VIP</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>Name</th>\n",
       "      <th>Transported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>B/0/P</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>39.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Maham Ofracculy</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>F/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>24.0</td>\n",
       "      <td>False</td>\n",
       "      <td>109.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>Juanna Vines</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0003_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>A/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>58.0</td>\n",
       "      <td>True</td>\n",
       "      <td>43.0</td>\n",
       "      <td>3576.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6715.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>Altark Susent</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0003_02</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>A/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>33.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>3329.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>Solam Susent</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0004_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>F/1/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>16.0</td>\n",
       "      <td>False</td>\n",
       "      <td>303.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Willy Santantines</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8688</th>\n",
       "      <td>9276_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>A/98/P</td>\n",
       "      <td>55 Cancri e</td>\n",
       "      <td>41.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6819.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1643.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>Gravior Noxnuther</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8689</th>\n",
       "      <td>9278_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>True</td>\n",
       "      <td>G/1499/S</td>\n",
       "      <td>PSO J318.5-22</td>\n",
       "      <td>18.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Kurta Mondalley</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8690</th>\n",
       "      <td>9279_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>G/1500/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>26.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1872.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Fayey Connon</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8691</th>\n",
       "      <td>9280_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>E/608/S</td>\n",
       "      <td>55 Cancri e</td>\n",
       "      <td>32.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1049.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>353.0</td>\n",
       "      <td>3235.0</td>\n",
       "      <td>Celeon Hontichre</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8692</th>\n",
       "      <td>9280_02</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>E/608/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>44.0</td>\n",
       "      <td>False</td>\n",
       "      <td>126.0</td>\n",
       "      <td>4688.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Propsh Hontichre</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8693 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId HomePlanet CryoSleep     Cabin    Destination   Age    VIP  \\\n",
       "0        0001_01     Europa     False     B/0/P    TRAPPIST-1e  39.0  False   \n",
       "1        0002_01      Earth     False     F/0/S    TRAPPIST-1e  24.0  False   \n",
       "2        0003_01     Europa     False     A/0/S    TRAPPIST-1e  58.0   True   \n",
       "3        0003_02     Europa     False     A/0/S    TRAPPIST-1e  33.0  False   \n",
       "4        0004_01      Earth     False     F/1/S    TRAPPIST-1e  16.0  False   \n",
       "...          ...        ...       ...       ...            ...   ...    ...   \n",
       "8688     9276_01     Europa     False    A/98/P    55 Cancri e  41.0   True   \n",
       "8689     9278_01      Earth      True  G/1499/S  PSO J318.5-22  18.0  False   \n",
       "8690     9279_01      Earth     False  G/1500/S    TRAPPIST-1e  26.0  False   \n",
       "8691     9280_01     Europa     False   E/608/S    55 Cancri e  32.0  False   \n",
       "8692     9280_02     Europa     False   E/608/S    TRAPPIST-1e  44.0  False   \n",
       "\n",
       "      RoomService  FoodCourt  ShoppingMall     Spa  VRDeck               Name  \\\n",
       "0             0.0        0.0           0.0     0.0     0.0    Maham Ofracculy   \n",
       "1           109.0        9.0          25.0   549.0    44.0       Juanna Vines   \n",
       "2            43.0     3576.0           0.0  6715.0    49.0      Altark Susent   \n",
       "3             0.0     1283.0         371.0  3329.0   193.0       Solam Susent   \n",
       "4           303.0       70.0         151.0   565.0     2.0  Willy Santantines   \n",
       "...           ...        ...           ...     ...     ...                ...   \n",
       "8688          0.0     6819.0           0.0  1643.0    74.0  Gravior Noxnuther   \n",
       "8689          0.0        0.0           0.0     0.0     0.0    Kurta Mondalley   \n",
       "8690          0.0        0.0        1872.0     1.0     0.0       Fayey Connon   \n",
       "8691          0.0     1049.0           0.0   353.0  3235.0   Celeon Hontichre   \n",
       "8692        126.0     4688.0           0.0     0.0    12.0   Propsh Hontichre   \n",
       "\n",
       "      Transported  \n",
       "0           False  \n",
       "1            True  \n",
       "2           False  \n",
       "3           False  \n",
       "4            True  \n",
       "...           ...  \n",
       "8688        False  \n",
       "8689        False  \n",
       "8690         True  \n",
       "8691        False  \n",
       "8692         True  \n",
       "\n",
       "[8693 rows x 14 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_path = UtilPath.get_root_path()\n",
    "\n",
    "df = pd.read_csv(root_path+'/datasets/spaceship-titanic/train.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializando os transformadores\n",
    "* NumpyToTorchTensor: Transforma um array numpy em um tensor torch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classe do transformador\n",
    "```python\n",
    "class NumpyToTorchTensor:\n",
    "    def __init__(self, dtype):\n",
    "        self.dtype = dtype\n",
    "    def __call__(self, sample ):\n",
    "        return torch.tensor(sample.tolist(),dtype=self.dtype)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_float = NumpyToTorchTensor(dtype=float)\n",
    "transform_int = NumpyToTorchTensor(dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-Processamento\n",
    "\n",
    "* Os dados das colunas PassengerId, HomePlanet, CryoSleep, Cabin, VIP, Name e Transported foram tratados como dados categóricos, enquanto as colunas restantes foram tratadas como dados númericos. A etapa de pré-processamento deste trabalho consiste da criação de vocabulários a partir das palavras de cada dado categórico, os quais posteriormente passarão por uma rede de __embedding__ para serem representados em um espaço de __d__ dimensões. Além disso, os dados das colunas Passenger_Id, Cabin e Name foram separados (__splitted__), respectivamente, pelos caracteres \"_\", \"/\" e \"  \", gerando mais informações. \n",
    "    * Por exemplo, se um passageiro é representado pelo Id \"0001_01\", o novo dado gerado será [\"0001\",\"01\"]. \n",
    "    * O vocábulário é representado por um dicionário que mapeia um texto em um valor numérico (str -> int).\n",
    "\n",
    "* Vale ressaltar que, como são gerados vocabulários a partir dos textos dos dados categóricos, é necessário estabelecer um vocábulo para representar os textos nunca vistos, ou seja, aqueles que não estão nos dados de treinamento, sendo este o vocábulo 'unknown'.\n",
    "\n",
    "* Outro ponto importante sobre o pré-processamento: os dados das colunas CryoSleep e VIP (com valor booleano) foram concatenados com o valor do vocábulo da sua coluna. Por ex., se para uma _feature_ existe um valor False em CryoSleep (com representação no vocabulário com valor 7) e um valor True (com representação no vocabulário com valor 8) e o texto 'CryoSleep' e 'VIP' são representados, respectivamente, pelos valores 534 e 533, os novos dados gerados nas colunas CryoSleep e VIP serão, respectivamente, [7,534] e [8,533]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classe abaixo contém o código utilizado para realizar o pré-processamento dos dados, tanto para o treinamento, quanto para o teste. Ao final do pré-processamento dos dados de treino, é retornada uma lista com todos os dados pré-processados, ou seja, os dados categóricos representados pelos respectivos valores gerados no vocabulário criado e já descompactados das suas formas de listas, mais os dados numéricos. O pré-processamento dos dados de teste ocorrem da mesma forma, com exceção de que, ao invés de criar um vocabulário, é utilizado o vocabulário já criado no pré-processamento dos dados de treinamento para transformar os dados categóricos.\n",
    "\n",
    "```python\n",
    "\n",
    "class PreProcessorSpaceship:\n",
    "    def __init__(self, csv_train_filename: str):\n",
    "        self.df = pd.read_csv(csv_train_filename)\n",
    "        self.pre_processed_df = None\n",
    "        self.vocab = None\n",
    "    \n",
    "    def get_columns_name(self):\n",
    "        return self.df.columns\n",
    "\n",
    "    def get_words_by_column(sel,df,column_name):\n",
    "        words = set()\n",
    "        for words_list in df[column_name]:\n",
    "            for word in words_list:\n",
    "                words.add(word)\n",
    "        return list(words)\n",
    "\n",
    "\n",
    "    def pre_process(self):\n",
    "        if self.pre_processed_df != None:\n",
    "            return self.pre_processed_df, self.vocab\n",
    "\n",
    "        pre_processed_df = self.df.copy() \n",
    "\n",
    "        dict_max_len = {}\n",
    "\n",
    "        other_columns_to_be_processed = ['PassengerId','Cabin','Name']\n",
    "        columns_to_be_processed = ['HomePlanet','Destination']\n",
    "        boolean_columns = [\"CryoSleep\", \"VIP\"]\n",
    "        remain_columns =[\"Age\",\"RoomService\",\"FoodCourt\",\"ShoppingMall\",\"Spa\",\"VRDeck\"]\n",
    "        \n",
    "        for column in remain_columns:\n",
    "            pre_processed_df[column] = pre_processed_df[column].fillna(-1) \n",
    "        \n",
    "        for column in boolean_columns + other_columns_to_be_processed + columns_to_be_processed + [\"Transported\"]:\n",
    "            pre_processed_df[column] = pre_processed_df[column].astype(str)\n",
    "\n",
    "        for column in remain_columns:\n",
    "            pre_processed_df[column] = pre_processed_df[column].astype(float)\n",
    "        \n",
    "        pre_processed_df[\"PassengerId\"] = UtilPreProcessor.split_column_data_by_pattern(pre_processed_df,\"PassengerId\",\"_\")\n",
    "        pre_processed_df[\"Cabin\"] = UtilPreProcessor.split_column_data_by_pattern(pre_processed_df,\"Cabin\",\"/\")\n",
    "        pre_processed_df[\"Name\"] = UtilPreProcessor.split_column_data_by_pattern(pre_processed_df,\"Name\",\" \")        \n",
    "\n",
    "        passenger_id_words = self.get_words_by_column(pre_processed_df,\"PassengerId\")\n",
    "        cabin_words = self.get_words_by_column(pre_processed_df,\"Cabin\")\n",
    "        name_words = self.get_words_by_column(pre_processed_df,\"Name\")\n",
    "        words = UtilPreProcessor.get_unique_values_by_columns(pre_processed_df,columns_to_be_processed + boolean_columns) + columns_to_be_processed  \\\n",
    "                                                    + boolean_columns + passenger_id_words + cabin_words + name_words + \\\n",
    "        [f\"{i}{j}{k}{l}\" for i in range(10) for j in range(10) for k in range(10) for l in range(10)] + [str(i) for i in range(10**4)] + ['unknown']\n",
    "        vocab_to_index, index_to_vocab = UtilPreProcessor.creat_vocab(words)\n",
    "        \n",
    "        for column in other_columns_to_be_processed:\n",
    "            listt = pre_processed_df[column].to_list()\n",
    "            max_len = max(len(sublist) for sublist in listt)\n",
    "            dict_max_len[column] = max_len\n",
    "\n",
    "        for column in other_columns_to_be_processed:\n",
    "            pre_processed_df[column] = pre_processed_df[column].apply(lambda x: [vocab_to_index[vocab] for vocab in x] if len(x) == dict_max_len[column] else [vocab_to_index['nan'] for _ in range(dict_max_len[column])])\n",
    "\n",
    "        for column in columns_to_be_processed:\n",
    "            pre_processed_df[column] = pre_processed_df[column].apply(lambda x: vocab_to_index[x])\n",
    "\n",
    "        for column in boolean_columns:\n",
    "            pre_processed_df[column] = pre_processed_df[column].apply(lambda x: [vocab_to_index[x],vocab_to_index[column]])\n",
    "\n",
    "        pre_processed_df['Transported'] = pre_processed_df['Transported'].apply(lambda x: vocab_to_index[x])\n",
    "        pre_processed_data = []\n",
    "        \n",
    "        for i,row in enumerate(pre_processed_df.itertuples(index=False)):\n",
    "            pre_processed_data.append([])\n",
    "            for data in (row):\n",
    "                if data.__class__ is list:\n",
    "                    pre_processed_data[i] += data\n",
    "                else:\n",
    "                    pre_processed_data[i].append(data)\n",
    "\n",
    "        self.vocab = [vocab_to_index,index_to_vocab]\n",
    "        self.pre_processed_df = pre_processed_df\n",
    "        return pre_processed_data, self.vocab\n",
    "    \n",
    "    def pre_process_test(self,csv_filename):\n",
    "        df_test = pd.read_csv(csv_filename)\n",
    "\n",
    "        pre_processed_df = df_test.copy() \n",
    "\n",
    "        dict_max_len = {}\n",
    "\n",
    "        other_columns_to_be_processed = ['PassengerId','Cabin','Name']\n",
    "        columns_to_be_processed = ['HomePlanet','Destination']\n",
    "        boolean_columns = [\"CryoSleep\", \"VIP\"]\n",
    "        remain_columns =[\"Age\",\"RoomService\",\"FoodCourt\",\"ShoppingMall\",\"Spa\",\"VRDeck\"]\n",
    "        \n",
    "        for column in remain_columns:\n",
    "            pre_processed_df[column] = pre_processed_df[column].fillna(-1) \n",
    "        \n",
    "        for column in boolean_columns + other_columns_to_be_processed + columns_to_be_processed:\n",
    "            pre_processed_df[column] = pre_processed_df[column].astype(str)\n",
    "\n",
    "        for column in remain_columns:\n",
    "            pre_processed_df[column] = pre_processed_df[column].astype(float)\n",
    "        \n",
    "        pre_processed_df[\"PassengerId\"] = UtilPreProcessor.split_column_data_by_pattern(pre_processed_df,\"PassengerId\",\"_\")\n",
    "        pre_processed_df[\"Cabin\"] = UtilPreProcessor.split_column_data_by_pattern(pre_processed_df,\"Cabin\",\"/\")\n",
    "        pre_processed_df[\"Name\"] = UtilPreProcessor.split_column_data_by_pattern(pre_processed_df,\"Name\",\" \")        \n",
    "\n",
    "        \n",
    "        for column in other_columns_to_be_processed:\n",
    "            listt = pre_processed_df[column].to_list()\n",
    "            max_len = max(len(sublist) for sublist in listt)\n",
    "            dict_max_len[column] = max_len\n",
    "\n",
    "        for column in other_columns_to_be_processed:\n",
    "            pre_processed_df[column] = pre_processed_df[column].apply(lambda x: [self.vocab[0][vocab] if self.vocab[0].get(vocab) != None else self.vocab[0]['unknown']  for vocab in x] if len(x) == dict_max_len[column] else [self.vocab[0]['nan'] for _ in range(dict_max_len[column])])\n",
    "\n",
    "        for column in columns_to_be_processed:\n",
    "            pre_processed_df[column] = pre_processed_df[column].apply(lambda x:self.vocab[0][x] if self.vocab[0].get(x) != None else self.vocab[0]['unknown'] )\n",
    "\n",
    "        for column in boolean_columns:\n",
    "            pre_processed_df[column] = pre_processed_df[column].apply(lambda x: [self.vocab[0][x] if self.vocab[0].get(x) != None else self.vocab[0]['unknown'] ,self.vocab[0][column]])\n",
    "\n",
    "        pre_processed_data = []\n",
    "        \n",
    "        for i,row in enumerate(pre_processed_df.itertuples(index=False)):\n",
    "            pre_processed_data.append([])\n",
    "            for data in (row):\n",
    "                if data.__class__ is list:\n",
    "                    pre_processed_data[i] += data\n",
    "                else:\n",
    "                    pre_processed_data[i].append(data)\n",
    "\n",
    "        return pre_processed_data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processando os dados de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processor = PreProcessorSpaceship(root_path+\"/datasets/spaceship-titanic/train.csv\")\n",
    "pre_processed_data, vocab = pre_processor.pre_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature original:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Age</th>\n",
       "      <th>VIP</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>Name</th>\n",
       "      <th>Transported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>B/0/P</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>39.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Maham Ofracculy</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PassengerId HomePlanet CryoSleep  Cabin  Destination   Age    VIP  \\\n",
       "0     0001_01     Europa     False  B/0/P  TRAPPIST-1e  39.0  False   \n",
       "\n",
       "   RoomService  FoodCourt  ShoppingMall  Spa  VRDeck             Name  \\\n",
       "0          0.0        0.0           0.0  0.0     0.0  Maham Ofracculy   \n",
       "\n",
       "   Transported  \n",
       "0        False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature pré-processada:\n",
      "[605, 1740, 0, 7, 11, 6400, 6766, 6834, 4, 39.0, 7, 12, 0.0, 0.0, 0.0, 0.0, 0.0, 12408, 10000, 7]\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature original:\")\n",
    "display(pre_processor.df.head(1))\n",
    "print('\\nFeature pré-processada:')\n",
    "print(pre_processed_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alguns itens no vocabulário gerado\n",
      "   Vocábulo: Europa - Valor associado: 0\n",
      "   Vocábulo: Earth - Valor associado: 1\n",
      "   Vocábulo: Mars - Valor associado: 2\n",
      "   Vocábulo: nan - Valor associado: 3\n",
      "   Vocábulo: TRAPPIST-1e - Valor associado: 4\n",
      "   Vocábulo: PSO J318.5-22 - Valor associado: 5\n",
      "   Vocábulo: 55 Cancri e - Valor associado: 6\n",
      "   Vocábulo: False - Valor associado: 7\n",
      "   Vocábulo: True - Valor associado: 8\n",
      "   Vocábulo: HomePlanet - Valor associado: 9\n"
     ]
    }
   ],
   "source": [
    "print('Alguns itens no vocabulário gerado')\n",
    "for k,v in list(vocab[0].items())[0:10]:\n",
    "    print(f'   Vocábulo: {k} - Valor associado: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processamento dos dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_test_data = pre_processor.pre_process_test(root_path+\"/datasets/spaceship-titanic/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação das classes dataset\n",
    "\n",
    "Para facilitar a manipulação dos dados do dataset foi criada a classe abaixo:\n",
    "\n",
    "```python\n",
    "class SpaceshipTitanicDataset(Dataset):\n",
    "    def __init__(self, data, transform = None,type ='train'):\n",
    "        if type == 'train':\n",
    "            self.y = np.array(data,dtype=float)[:,-1]\n",
    "            self.x = np.array(data,dtype=float)[:,:-1]\n",
    "        else:\n",
    "            self.x = np.array(data,dtype=float)\n",
    "            self.y = np.array(data,dtype=float)[:,-1]\n",
    "        self.transform = transform\n",
    "        self.len = len(self.x)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        if self.transform:\n",
    "            return self.transform(self.x[index]),self.transform(self.y[index])\n",
    "        return self.x[index],self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação dos datasets de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SpaceshipTitanicDataset(pre_processed_data,transform=transform_float)\n",
    "test_dataset = SpaceshipTitanicDataset(pre_processed_test_data,transform=transform_float,type='eval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O modelo\n",
    "\n",
    "O modelo consiste de uma rede neural com uma camada de Embedding para criar representações para os dados categóricos transformados a partir do vocabulário criado na etapa de pré-processamento e uma camada Linear para transformar todas as features geradas em uma representação de __d__ dimensões. Apesar de ser um problema de classificação, o modelo é treinado para gerar uma representação que seja próxima das representações de \"True\" ou \"False\" de acordo com os labels fornecidos. Dessa forma, se a predição está mais próxima da representação de \"True\", então a resposta do modelo é mapeado para True, caso contrário, False. Além disso, o modelo conta com uma camada de dropout e uma função não linear LeakyReLU, como visto na classe abaixo.\n",
    "\n",
    "Os parâmetros do modelo são:\n",
    "* __dim_total_data__: Quantidade de features de cada passageiro após a etapa de pré-processamento;\n",
    "* __dim_total_vocab_data__: Quantidade de features de cada passageiro que passarão pela rede de embedding;\n",
    "* __len_vocab__: Quantidade de palavras no vocabulário criado na etapa de pré-processamento;\n",
    "* __dim_embedd__: Dimensão das representações geradas pela rede de Embedding;\n",
    "* __dropout__: Taxa de dropout aplicada na rede."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class SpaceShipModel(nn.Module):\n",
    "    def __init__(self,dim_total_data,dim_total_vocab_data,len_vocab,dim_embedd,dropout = 0.5):\n",
    "        super(SpaceShipModel,self).__init__()\n",
    "        self.embedding = nn.Embedding(len_vocab,dim_embedd,dtype=float)\n",
    "        self.linear = nn.Linear((dim_total_data-dim_total_vocab_data) + dim_total_vocab_data*dim_embedd,dim_embedd,dtype=float)\n",
    "        self.dim_total_vocab_data = dim_total_vocab_data\n",
    "        self.dim_embedd = dim_embedd\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    \n",
    "    def forward(self, features_embedding, remain_features,batch_size):\n",
    "        x = self.embedding(features_embedding)\n",
    "        x = x.reshape((batch_size,self.dim_total_vocab_data*self.dim_embedd))\n",
    "        x = self.dropout(x)\n",
    "        return  self.leaky_relu(self.linear(torch.concat([x,remain_features],dim=1)))\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estrutura criada para automatizar cálculos necessários\n",
    "# Keys: Nome das colunas categóricas\n",
    "# Values:\n",
    "#     * [0] índice inicial da coluna \n",
    "#     * [1] quantidade de dados que esta coluna contém\n",
    "vocab_columns = {\n",
    "    \"PassengerId\": [0,2],\n",
    "    \"HomePlanet\": [1,1],\n",
    "    \"CryoSleep\": [2,2],\n",
    "    \"Cabin\": [3,3],\n",
    "    \"VIP\": [6,2],\n",
    "    \"Name\": [12,2]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definindo os hyper-parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_vocab = len(vocab[0])\n",
    "dim_total_data = len(pre_processed_data[0]) - 1\n",
    "sum_values = lambda x: sum(v[1] for v in x)\n",
    "dim_total_vocab_data = sum_values(list(vocab_columns.values()))\n",
    "\n",
    "dim_embedd = 32\n",
    "batch_size = 32 \n",
    "epochs = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SpaceShipModel(dim_total_data,dim_total_vocab_data,len_vocab,dim_embedd)\n",
    "optim = torch.optim.Adam(model.parameters(),0.001)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função auxiliar\n",
    "\n",
    "A função abaixo separa as features em: \n",
    "1. Features que serão utilizadas na rede de embedding; \n",
    "2. Features que não serão utilizadas na rede de embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_features(features,transform_int:NumpyToTorchTensor,transform_float:NumpyToTorchTensor):\n",
    "    indexes = []\n",
    "    shift = 0\n",
    "    for k,v in vocab_columns.values():\n",
    "        for i in range(k+shift,k+v+shift):\n",
    "            indexes.append(i)\n",
    "        shift += v-1\n",
    "    features_embedding = []\n",
    "    remain_features = []\n",
    "    with torch.no_grad():\n",
    "        for i,passenger_feature in enumerate(features):\n",
    "            features_embedding.append([])\n",
    "            remain_features.append([])\n",
    "            for j,feature in enumerate(passenger_feature.tolist()):\n",
    "                if j in indexes:\n",
    "                    features_embedding[i].append(feature)\n",
    "                else:\n",
    "                    remain_features[i].append(feature)\n",
    "    return transform_int(np.array(features_embedding)),transform_float(np.array(remain_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 - loss 1.726446131580556\n",
      "epoch 2 - loss 0.6365498661807065\n",
      "epoch 3 - loss 0.37899341563218353\n",
      "epoch 4 - loss 0.2717677503222153\n",
      "epoch 5 - loss 0.24237342796723851\n",
      "epoch 6 - loss 0.18287956253968454\n",
      "epoch 7 - loss 0.13302804194034654\n",
      "epoch 8 - loss 0.1956914086786319\n",
      "epoch 9 - loss 0.07722939071826179\n",
      "epoch 10 - loss 0.06819934350451132\n",
      "epoch 11 - loss 0.044708854885117236\n",
      "epoch 12 - loss 0.07512131463935634\n",
      "epoch 13 - loss 0.08210265337571362\n",
      "epoch 14 - loss 0.033186317532919576\n",
      "epoch 15 - loss 0.13782333217056258\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(train_dataset,batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for features,target in dataloader:\n",
    "        embedding_features, remain_features = process_features(features, transform_int,transform_float)\n",
    "        optim.zero_grad()\n",
    "        predictions = model(embedding_features,remain_features,len(features))\n",
    "        \n",
    "        target = transform_int(target)\n",
    "        embedd_target = model.embedding(target)\n",
    "        \n",
    "        loss = loss_fn(predictions, embedd_target)\n",
    "        loss.backward()\n",
    "        \n",
    "        optim.step()\n",
    "\n",
    "    print(f\"epoch {epoch + 1} - loss {loss}\")\n",
    "\n",
    "torch.save(model.state_dict(), root_path+'/checkpoints/spaceship_titanic_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerando resultados para submissão no Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "df_sample_submission = pd.DataFrame({\"PassengerId\":[],\"Transported\":[]}) \n",
    "\n",
    "embedd_true = model.embedding(transform_int(np.array(vocab[0]['True'])))\n",
    "embedd_true = embedd_true.reshape((1,dim_embedd))\n",
    "\n",
    "embedd_false = model.embedding(transform_int(np.array(vocab[0]['False'])))\n",
    "embedd_false = embedd_false.reshape((1,dim_embedd))\n",
    "for i in range(len(test_dataset)):\n",
    "    x,_= test_dataset[i]\n",
    "    embedding_features, remain_features = process_features([x], transform_int,transform_float)\n",
    "    prediction = model(embedding_features,remain_features,1)\n",
    "    mse_true = loss_fn(prediction,embedd_true)\n",
    "    mse_false = loss_fn(prediction,embedd_false)\n",
    "    if mse_false < mse_true:\n",
    "        pred = \"False\"\n",
    "    else:\n",
    "        pred = \"True\"\n",
    "    string = f\"{vocab[1][int(x[0].item())]}_{vocab[1][int(x[1].item())]}\"\n",
    "    dict_data = {\n",
    "        \"PassengerId\":[string],\n",
    "        \"Transported\": pred\n",
    "    }\n",
    "    temp_df  = pd.DataFrame(dict_data)\n",
    "    df_sample_submission = pd.concat([temp_df,df_sample_submission],ignore_index =True)\n",
    "df_sample_submission.to_csv(root_path+\"/datasets/spaceship-titanic/sample_submission.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados das submissões\n",
    "\n",
    "__Nome de usuário do Kaggle: fabiotrindaderamos.__\n",
    "\n",
    "Foram realizados variações nos hyperparâmetros com o objetivo de aumentar a precião do modelo. A imagem abaixo apresenta os resultados obtidos nas diferentes tentativas:\n",
    "\n",
    "![image.png](../../images/score_kaggle_tp1.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
