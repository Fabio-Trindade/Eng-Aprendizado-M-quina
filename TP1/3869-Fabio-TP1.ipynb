{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuração Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from -r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements.txt (line 1)) (2.2.1)\n",
      "Requirement already satisfied: scikit-learn in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from -r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements.txt (line 2)) (1.4.1.post1)\n",
      "Requirement already satisfied: joblib in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from -r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements.txt (line 3)) (1.3.2)\n",
      "Requirement already satisfied: filelock in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements.txt (line 1)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements.txt (line 1)) (4.10.0)\n",
      "Requirement already satisfied: sympy in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: networkx in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements.txt (line 1)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements.txt (line 1)) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements.txt (line 1)) (2024.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements.txt (line 1)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements.txt (line 1)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements.txt (line 1)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements.txt (line 1)) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements.txt (line 1)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements.txt (line 1)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements.txt (line 1)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements.txt (line 1)) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements.txt (line 1)) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements.txt (line 1)) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements.txt (line 1)) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements.txt (line 1)) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements.txt (line 1)) (12.4.99)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from scikit-learn->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from scikit-learn->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements.txt (line 2)) (1.12.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from scikit-learn->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements.txt (line 2)) (3.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from jinja2->torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements.txt (line 1)) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/fabio/venvs/my_venv/lib/python3.11/site-packages (from sympy->torch->-r /home/fabio/Mestrado/Eng-Aprendizado-Maquina/requirements.txt (line 1)) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "if str(pathlib.Path().resolve().name) == \"TP1\":\n",
    "    root = str(pathlib.Path().resolve().parent)+'/'\n",
    "    sys.path.append(root)\n",
    "    requirements_path = root + 'requirements.txt'\n",
    "else:\n",
    "    ! git clone https://github.com/Fabio-Trindade/Eng-Aprendizado-Maquina.git\n",
    "    root = str(pathlib.Path().resolve())\n",
    "    src_path = '/Eng-Aprendizado-Maquina/'\n",
    "    requirements_path = src_path + 'requirements.txt'\n",
    "    sys.path.append(root + '/Eng-Aprendizado-Maquina/')\n",
    "\n",
    "! pip install -r $requirements_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.datasets.spaceship_titanic_dataset import SpaceshipTitanicDataset\n",
    "from src.models.spaceship_model import SpaceShipModel\n",
    "from src.pre_processors.pre_processor_spaceship import PreProcessorSpaceship\n",
    "from src.transforms.to_torch_tensor import NumpyToTorchTensor\n",
    "from src.utils.util_path import UtilPath\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Os dados\n",
    "\n",
    "Some description..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Age</th>\n",
       "      <th>VIP</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>Name</th>\n",
       "      <th>Transported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>B/0/P</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>39.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Maham Ofracculy</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>F/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>24.0</td>\n",
       "      <td>False</td>\n",
       "      <td>109.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>Juanna Vines</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0003_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>A/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>58.0</td>\n",
       "      <td>True</td>\n",
       "      <td>43.0</td>\n",
       "      <td>3576.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6715.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>Altark Susent</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0003_02</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>A/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>33.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>3329.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>Solam Susent</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0004_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>F/1/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>16.0</td>\n",
       "      <td>False</td>\n",
       "      <td>303.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Willy Santantines</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8688</th>\n",
       "      <td>9276_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>A/98/P</td>\n",
       "      <td>55 Cancri e</td>\n",
       "      <td>41.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6819.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1643.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>Gravior Noxnuther</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8689</th>\n",
       "      <td>9278_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>True</td>\n",
       "      <td>G/1499/S</td>\n",
       "      <td>PSO J318.5-22</td>\n",
       "      <td>18.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Kurta Mondalley</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8690</th>\n",
       "      <td>9279_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>G/1500/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>26.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1872.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Fayey Connon</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8691</th>\n",
       "      <td>9280_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>E/608/S</td>\n",
       "      <td>55 Cancri e</td>\n",
       "      <td>32.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1049.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>353.0</td>\n",
       "      <td>3235.0</td>\n",
       "      <td>Celeon Hontichre</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8692</th>\n",
       "      <td>9280_02</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>E/608/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>44.0</td>\n",
       "      <td>False</td>\n",
       "      <td>126.0</td>\n",
       "      <td>4688.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Propsh Hontichre</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8693 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId HomePlanet CryoSleep     Cabin    Destination   Age    VIP  \\\n",
       "0        0001_01     Europa     False     B/0/P    TRAPPIST-1e  39.0  False   \n",
       "1        0002_01      Earth     False     F/0/S    TRAPPIST-1e  24.0  False   \n",
       "2        0003_01     Europa     False     A/0/S    TRAPPIST-1e  58.0   True   \n",
       "3        0003_02     Europa     False     A/0/S    TRAPPIST-1e  33.0  False   \n",
       "4        0004_01      Earth     False     F/1/S    TRAPPIST-1e  16.0  False   \n",
       "...          ...        ...       ...       ...            ...   ...    ...   \n",
       "8688     9276_01     Europa     False    A/98/P    55 Cancri e  41.0   True   \n",
       "8689     9278_01      Earth      True  G/1499/S  PSO J318.5-22  18.0  False   \n",
       "8690     9279_01      Earth     False  G/1500/S    TRAPPIST-1e  26.0  False   \n",
       "8691     9280_01     Europa     False   E/608/S    55 Cancri e  32.0  False   \n",
       "8692     9280_02     Europa     False   E/608/S    TRAPPIST-1e  44.0  False   \n",
       "\n",
       "      RoomService  FoodCourt  ShoppingMall     Spa  VRDeck               Name  \\\n",
       "0             0.0        0.0           0.0     0.0     0.0    Maham Ofracculy   \n",
       "1           109.0        9.0          25.0   549.0    44.0       Juanna Vines   \n",
       "2            43.0     3576.0           0.0  6715.0    49.0      Altark Susent   \n",
       "3             0.0     1283.0         371.0  3329.0   193.0       Solam Susent   \n",
       "4           303.0       70.0         151.0   565.0     2.0  Willy Santantines   \n",
       "...           ...        ...           ...     ...     ...                ...   \n",
       "8688          0.0     6819.0           0.0  1643.0    74.0  Gravior Noxnuther   \n",
       "8689          0.0        0.0           0.0     0.0     0.0    Kurta Mondalley   \n",
       "8690          0.0        0.0        1872.0     1.0     0.0       Fayey Connon   \n",
       "8691          0.0     1049.0           0.0   353.0  3235.0   Celeon Hontichre   \n",
       "8692        126.0     4688.0           0.0     0.0    12.0   Propsh Hontichre   \n",
       "\n",
       "      Transported  \n",
       "0           False  \n",
       "1            True  \n",
       "2           False  \n",
       "3           False  \n",
       "4            True  \n",
       "...           ...  \n",
       "8688        False  \n",
       "8689        False  \n",
       "8690         True  \n",
       "8691        False  \n",
       "8692         True  \n",
       "\n",
       "[8693 rows x 14 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_path = UtilPath.get_root_path()\n",
    "\n",
    "df = pd.read_csv(root_path+'/datasets/spaceship-titanic/train.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializando os transformadores\n",
    "A classe abaixo apenas transformam um array numpy em um tensor do torch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classe do transformador\n",
    "```python\n",
    "class NumpyToTorchTensor:\n",
    "    def __init__(self, dtype):\n",
    "        self.dtype = dtype\n",
    "    def __call__(self, sample ):\n",
    "        return torch.tensor(sample.tolist(),dtype=self.dtype)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_float = NumpyToTorchTensor(dtype=float)\n",
    "transform_int = NumpyToTorchTensor(dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-Processamento\n",
    "\n",
    "Os dados das colunas PassengerId, HomePlanet, CryoSleep, Cabin, VIP, Name e Transported foram tratados como dados categóricos, enquanto as colunas restantes foram tratadas como dados númericos. A etapa de pré-processamento deste trabalho consiste da criação de vocábulários a partir das palavras ed cada dado categórico, os quais posteriormente passarão por uma rede de __embedding__ para serem representados em um espaço de __d__ dimensões. Além disso, os dados das colunas Passenger_Id, Cabin e Name foram separados (__splitted__), respectivamente, pelos caracteres \"_\", \"/\" e \" \", gerando mais informações. Por exemplo, se um passageiro é representado pelo Id \"0001_01\", o novo dado gerado será [\"0001\",\"01\"]. Já o vocábulário é representado por um dicionário que mapeia um texto em um valor numérico.\n",
    "\n",
    "Vale ressaltar que, como são gerados vocabulários a partir dos textos dos dados categóricos, é necessário estabelecer um vocábulo para representar os textos nunca vistos, ou seja, aqueles que não estão nos dados de treinamento, sendo este o vocábulo 'unknown'.\n",
    "\n",
    "Outro ponto importante sobre o pré-processamento: os dados das colunas CryoSleep e VIP (com valor booleano) foram concatenados com o valor do vocábulo do nome da sua coluna. Por ex., se para uma __feature__ existe um valor False em CryoSleep (com representação no vocabulário com valor 7) e um valor True (com representação no vocabulário com valor 8) e o texto 'CryoSleep' e 'VIP' possuem, respectivamente, o valor 534 e 533, o novo dado gerado na coluna CryoSleep e VIP serão, respectivamente, [7,534] e [8,533].\n",
    "\n",
    "A classe abaixo contém o código utilizado para realizar o pré-processamento dos dados, tanto para o treinamento, quanto para o teste. Ao final do pré-processamento dos dados de treino, é retornada uma lista com todos os dados pré-processados, ou seja, os dados categóricos representados pelos respectivos valores gerados no vocabulário criado e já descompactados das suas formas de listas. O pré-processamento dos dados de teste ocorrem da mesma forma, com exceção de que, ao invés de criar um vocabulário, é utilizado o vocabulário já criado no pré-processamento dos dados de treinamento para transformar os dados categóricos em seus respectivo valor numérico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "class PreProcessorSpaceship:\n",
    "    def __init__(self, csv_train_filename: str):\n",
    "        self.df = pd.read_csv(csv_train_filename)\n",
    "        self.pre_processed_df = None\n",
    "        self.vocab = None\n",
    "    \n",
    "    def get_columns_name(self):\n",
    "        return self.df.columns\n",
    "\n",
    "    def get_words_by_column(sel,df,column_name):\n",
    "        words = set()\n",
    "        for words_list in df[column_name]:\n",
    "            for word in words_list:\n",
    "                words.add(word)\n",
    "        return list(words)\n",
    "\n",
    "\n",
    "    def pre_process(self):\n",
    "        if self.pre_processed_df != None:\n",
    "            return self.pre_processed_df, self.vocab\n",
    "\n",
    "        pre_processed_df = self.df.copy() \n",
    "\n",
    "        dict_max_len = {}\n",
    "\n",
    "        other_columns_to_be_processed = ['PassengerId','Cabin','Name']\n",
    "        columns_to_be_processed = ['HomePlanet','Destination']\n",
    "        boolean_columns = [\"CryoSleep\", \"VIP\"]\n",
    "        remain_columns =[\"Age\",\"RoomService\",\"FoodCourt\",\"ShoppingMall\",\"Spa\",\"VRDeck\"]\n",
    "        \n",
    "        for column in remain_columns:\n",
    "            pre_processed_df[column] = pre_processed_df[column].fillna(-1) \n",
    "        \n",
    "        for column in boolean_columns + other_columns_to_be_processed + columns_to_be_processed + [\"Transported\"]:\n",
    "            pre_processed_df[column] = pre_processed_df[column].astype(str)\n",
    "\n",
    "        for column in remain_columns:\n",
    "            pre_processed_df[column] = pre_processed_df[column].astype(float)\n",
    "        \n",
    "        pre_processed_df[\"PassengerId\"] = UtilPreProcessor.split_column_data_by_pattern(pre_processed_df,\"PassengerId\",\"_\")\n",
    "        pre_processed_df[\"Cabin\"] = UtilPreProcessor.split_column_data_by_pattern(pre_processed_df,\"Cabin\",\"/\")\n",
    "        pre_processed_df[\"Name\"] = UtilPreProcessor.split_column_data_by_pattern(pre_processed_df,\"Name\",\" \")        \n",
    "\n",
    "        passenger_id_words = self.get_words_by_column(pre_processed_df,\"PassengerId\")\n",
    "        cabin_words = self.get_words_by_column(pre_processed_df,\"Cabin\")\n",
    "        name_words = self.get_words_by_column(pre_processed_df,\"Name\")\n",
    "        words = UtilPreProcessor.get_unique_values_by_columns(pre_processed_df,columns_to_be_processed + boolean_columns) + columns_to_be_processed  \\\n",
    "                                                    + boolean_columns + passenger_id_words + cabin_words + name_words + \\\n",
    "        [f\"{i}{j}{k}{l}\" for i in range(10) for j in range(10) for k in range(10) for l in range(10)] + [str(i) for i in range(10**4)] + ['unknown']\n",
    "        vocab_to_index, index_to_vocab = UtilPreProcessor.creat_vocab(words)\n",
    "        \n",
    "        for column in other_columns_to_be_processed:\n",
    "            listt = pre_processed_df[column].to_list()\n",
    "            max_len = max(len(sublist) for sublist in listt)\n",
    "            dict_max_len[column] = max_len\n",
    "\n",
    "        for column in other_columns_to_be_processed:\n",
    "            pre_processed_df[column] = pre_processed_df[column].apply(lambda x: [vocab_to_index[vocab] for vocab in x] if len(x) == dict_max_len[column] else [vocab_to_index['nan'] for _ in range(dict_max_len[column])])\n",
    "\n",
    "        for column in columns_to_be_processed:\n",
    "            pre_processed_df[column] = pre_processed_df[column].apply(lambda x: vocab_to_index[x])\n",
    "\n",
    "        for column in boolean_columns:\n",
    "            pre_processed_df[column] = pre_processed_df[column].apply(lambda x: [vocab_to_index[x],vocab_to_index[column]])\n",
    "\n",
    "        pre_processed_df['Transported'] = pre_processed_df['Transported'].apply(lambda x: vocab_to_index[x])\n",
    "        pre_processed_data = []\n",
    "        \n",
    "        for i,row in enumerate(pre_processed_df.itertuples(index=False)):\n",
    "            pre_processed_data.append([])\n",
    "            for data in (row):\n",
    "                if data.__class__ is list:\n",
    "                    pre_processed_data[i] += data\n",
    "                else:\n",
    "                    pre_processed_data[i].append(data)\n",
    "\n",
    "        self.vocab = [vocab_to_index,index_to_vocab]\n",
    "        self.pre_processed_df = pre_processed_df\n",
    "        return pre_processed_data, self.vocab\n",
    "    \n",
    "    def pre_process_test(self,csv_filename):\n",
    "        df_test = pd.read_csv(csv_filename)\n",
    "\n",
    "        pre_processed_df = df_test.copy() \n",
    "\n",
    "        dict_max_len = {}\n",
    "\n",
    "        other_columns_to_be_processed = ['PassengerId','Cabin','Name']\n",
    "        columns_to_be_processed = ['HomePlanet','Destination']\n",
    "        boolean_columns = [\"CryoSleep\", \"VIP\"]\n",
    "        remain_columns =[\"Age\",\"RoomService\",\"FoodCourt\",\"ShoppingMall\",\"Spa\",\"VRDeck\"]\n",
    "        \n",
    "        for column in remain_columns:\n",
    "            pre_processed_df[column] = pre_processed_df[column].fillna(-1) \n",
    "        \n",
    "        for column in boolean_columns + other_columns_to_be_processed + columns_to_be_processed:\n",
    "            pre_processed_df[column] = pre_processed_df[column].astype(str)\n",
    "\n",
    "        for column in remain_columns:\n",
    "            pre_processed_df[column] = pre_processed_df[column].astype(float)\n",
    "        \n",
    "        pre_processed_df[\"PassengerId\"] = UtilPreProcessor.split_column_data_by_pattern(pre_processed_df,\"PassengerId\",\"_\")\n",
    "        pre_processed_df[\"Cabin\"] = UtilPreProcessor.split_column_data_by_pattern(pre_processed_df,\"Cabin\",\"/\")\n",
    "        pre_processed_df[\"Name\"] = UtilPreProcessor.split_column_data_by_pattern(pre_processed_df,\"Name\",\" \")        \n",
    "\n",
    "        \n",
    "        for column in other_columns_to_be_processed:\n",
    "            listt = pre_processed_df[column].to_list()\n",
    "            max_len = max(len(sublist) for sublist in listt)\n",
    "            dict_max_len[column] = max_len\n",
    "\n",
    "        for column in other_columns_to_be_processed:\n",
    "            pre_processed_df[column] = pre_processed_df[column].apply(lambda x: [self.vocab[0][vocab] if self.vocab[0].get(vocab) != None else self.vocab[0]['unknown']  for vocab in x] if len(x) == dict_max_len[column] else [self.vocab[0]['nan'] for _ in range(dict_max_len[column])])\n",
    "\n",
    "        for column in columns_to_be_processed:\n",
    "            pre_processed_df[column] = pre_processed_df[column].apply(lambda x:self.vocab[0][x] if self.vocab[0].get(x) != None else self.vocab[0]['unknown'] )\n",
    "\n",
    "        for column in boolean_columns:\n",
    "            pre_processed_df[column] = pre_processed_df[column].apply(lambda x: [self.vocab[0][x] if self.vocab[0].get(x) != None else self.vocab[0]['unknown'] ,self.vocab[0][column]])\n",
    "\n",
    "        pre_processed_data = []\n",
    "        \n",
    "        for i,row in enumerate(pre_processed_df.itertuples(index=False)):\n",
    "            pre_processed_data.append([])\n",
    "            for data in (row):\n",
    "                if data.__class__ is list:\n",
    "                    pre_processed_data[i] += data\n",
    "                else:\n",
    "                    pre_processed_data[i].append(data)\n",
    "\n",
    "        return pre_processed_data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processando os dados de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processor = PreProcessorSpaceship(root_path+\"/datasets/spaceship-titanic/train.csv\")\n",
    "pre_processed_data, vocab = pre_processor.pre_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature original:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Age</th>\n",
       "      <th>VIP</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>Name</th>\n",
       "      <th>Transported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>B/0/P</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>39.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Maham Ofracculy</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PassengerId HomePlanet CryoSleep  Cabin  Destination   Age    VIP  \\\n",
       "0     0001_01     Europa     False  B/0/P  TRAPPIST-1e  39.0  False   \n",
       "\n",
       "   RoomService  FoodCourt  ShoppingMall  Spa  VRDeck             Name  \\\n",
       "0          0.0        0.0           0.0  0.0     0.0  Maham Ofracculy   \n",
       "\n",
       "   Transported  \n",
       "0        False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature pré-processada:\n",
      "[5296, 5550, 0, 7, 11, 6291, 6900, 7260, 4, 39.0, 7, 12, 0.0, 0.0, 0.0, 0.0, 0.0, 9099, 10486, 7]\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature original:\")\n",
    "display(pre_processor.df.head(1))\n",
    "print('Feature pré-processada:')\n",
    "print(pre_processed_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processamento dos dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_test_data = pre_processor.pre_process_test(root_path+\"/datasets/spaceship-titanic/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação das classes dataset\n",
    "\n",
    "Para facilitar a manipulação dos dados do dataset foi a classe abaixo:\n",
    "\n",
    "```python\n",
    "class SpaceshipTitanicDataset(Dataset):\n",
    "    def __init__(self, data, transform = None,type ='train'):\n",
    "        if type == 'train':\n",
    "            self.y = np.array(data,dtype=float)[:,-1]\n",
    "            self.x = np.array(data,dtype=float)[:,:-1]\n",
    "        else:\n",
    "            self.x = np.array(data,dtype=float)\n",
    "            self.y = np.array(data,dtype=float)[:,-1]\n",
    "        self.transform = transform\n",
    "        self.len = len(self.x)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        if self.transform:\n",
    "            return self.transform(self.x[index]),self.transform(self.y[index])\n",
    "        return self.x[index],self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação dos datasets de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SpaceshipTitanicDataset(pre_processed_data,transform=transform_float)\n",
    "test_dataset = SpaceshipTitanicDataset(pre_processed_test_data,transform=transform_float,type='eval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O modelo\n",
    "\n",
    "O modelo é uma rede neural que consiste de uma camada de Embedding para criar representações para os dados categóricos transformados a partir do vocabulário criado na etapa de pré-processamento e uma camada Linear para transformar as features geradas após o embedding junto com as features numéricas em uma representação de __d__ dimensões. Apesar de ser um problema de classificação, o modelo é treinado para gerar uma representação que sejam próximas das representações de \"True\" ou \"False\" de acordo com os labels fornecidos. Dessa forma, se a predição está mais próxima da representação de \"True\", então a respsota do modelo é mapeado para True, caso contrário, False. Além disso, o modelo conta com uma camada de dropout e uma função não linear LeakyReLU, como visto abaixo.\n",
    "\n",
    "Os parâmetros do modelo são:\n",
    "* a\n",
    "* b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class SpaceShipModel(nn.Module):\n",
    "    def __init__(self,dim_total_data,dim_total_vocab_data,len_vocab,dim_embedd,dropout = 0.5):\n",
    "        super(SpaceShipModel,self).__init__()\n",
    "        self.embedding = nn.Embedding(len_vocab,dim_embedd,dtype=float)\n",
    "        self.linear = nn.Linear((dim_total_data-dim_total_vocab_data) + dim_total_vocab_data*dim_embedd,dim_embedd,dtype=float)\n",
    "        self.dim_total_vocab_data = dim_total_vocab_data\n",
    "        self.dim_embedd = dim_embedd\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    \n",
    "    def forward(self, features_embedding, remain_features,batch_size):\n",
    "        x = self.embedding(features_embedding)\n",
    "        x = x.reshape((batch_size,self.dim_total_vocab_data*self.dim_embedd))\n",
    "        x = self.dropout(x)\n",
    "        return  self.leaky_relu(self.linear(torch.concat([x,remain_features],dim=1)))\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definindo os hyperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estrutura criada para automatizar cálculos necessários\n",
    "# Contém o nome das colunas categóricas como chave\n",
    "# Seus valores são, respectivamente, o índice inicial da coluna e a quantidade de dados que esta coluna contém\n",
    "\n",
    "vocab_columns = {\n",
    "    \"PassengerId\": [0,2],\n",
    "    \"HomePlanet\": [1,1],\n",
    "    \"CryoSleep\": [2,2],\n",
    "    \"Cabin\": [3,3],\n",
    "    \"VIP\": [6,2],\n",
    "    \"Name\": [12,2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_vocab = len(vocab[0])\n",
    "dim_total_data = len(pre_processed_data[0]) - 1\n",
    "sum_values = lambda x: sum(v[1] for v in x)\n",
    "dim_total_vocab_data = sum_values(list(vocab_columns.values()))\n",
    "\n",
    "dim_embedd = 16\n",
    "batch_size = 32 \n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SpaceShipModel(dim_total_data,dim_total_vocab_data,len_vocab,dim_embedd)\n",
    "optim = torch.optim.Adam(model.parameters(),0.001)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função auxiliar\n",
    "\n",
    "A função abaixo separa as features em features que serão utilizadas na rede de embedding e as que não serão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_features(features,transform_int:NumpyToTorchTensor,transform_float:NumpyToTorchTensor):\n",
    "    indexes = []\n",
    "    shift = 0\n",
    "    for k,v in vocab_columns.values():\n",
    "        for i in range(k+shift,k+v+shift):\n",
    "            indexes.append(i)\n",
    "        shift += v-1\n",
    "    features_embedding = []\n",
    "    remain_features = []\n",
    "    with torch.no_grad():\n",
    "        for i,passenger_feature in enumerate(features):\n",
    "            features_embedding.append([])\n",
    "            remain_features.append([])\n",
    "            for j,feature in enumerate(passenger_feature.tolist()):\n",
    "                if j in indexes:\n",
    "                    features_embedding[i].append(feature)\n",
    "                else:\n",
    "                    remain_features[i].append(feature)\n",
    "    return transform_int(np.array(features_embedding)),transform_float(np.array(remain_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 - loss 0.0007802485242828394\n",
      "epoch 2 - loss 0.000602535412162027\n",
      "epoch 3 - loss 0.00048543768473689167\n",
      "epoch 4 - loss 0.0004431209738913466\n",
      "epoch 5 - loss 0.00039962697735851393\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(train_dataset,batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for features,target in dataloader:\n",
    "        embedding_features, remain_features = process_features(features, transform_int,transform_float)\n",
    "        optim.zero_grad()\n",
    "        predictions = model(embedding_features,remain_features,len(features))\n",
    "        \n",
    "        target = transform_int(target)\n",
    "        embedd_target = model.embedding(target)\n",
    "        \n",
    "        loss = loss_fn(predictions, embedd_target)\n",
    "        loss.backward()\n",
    "        \n",
    "        optim.step()\n",
    "\n",
    "    calc = len(train_dataset)/batch_size\n",
    "    print(f\"epoch {epoch + 1} - loss {loss/calc}\")\n",
    "\n",
    "torch.save(model.state_dict(), root_path+'/checkpoints/spaceship_titanic_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerando resultados para submissão no Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "df_sample_submission = pd.DataFrame({\"PassengerId\":[],\"Transported\":[]}) \n",
    "\n",
    "embedd_true = model.embedding(transform_int(np.array(vocab[0]['True'])))\n",
    "embedd_true = embedd_true.reshape((1,dim_embedd))\n",
    "\n",
    "embedd_false = model.embedding(transform_int(np.array(vocab[0]['False'])))\n",
    "embedd_false = embedd_false.reshape((1,dim_embedd))\n",
    "for i in range(len(test_dataset)):\n",
    "    x,_= test_dataset[i]\n",
    "    embedding_features, remain_features = process_features([x], transform_int,transform_float)\n",
    "    prediction = model(embedding_features,remain_features,1)\n",
    "    mse_true = loss_fn(prediction,embedd_true)\n",
    "    mse_false = loss_fn(prediction,embedd_false)\n",
    "    if mse_false < mse_true:\n",
    "        pred = \"False\"\n",
    "    else:\n",
    "        pred = \"True\"\n",
    "    string = f\"{vocab[1][int(x[0].item())]}_{vocab[1][int(x[1].item())]}\"\n",
    "    dict_data = {\n",
    "        \"PassengerId\":[string],\n",
    "        \"Transported\": pred\n",
    "    }\n",
    "    temp_df  = pd.DataFrame(dict_data)\n",
    "    df_sample_submission = pd.concat([temp_df,df_sample_submission],ignore_index =True)\n",
    "df_sample_submission.to_csv(root_path+\"/datasets/spaceship-titanic/sample_submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados das submissões"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
